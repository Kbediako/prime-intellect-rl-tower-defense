# Prime RL hosted training (auto-advance, round-15/20 snapshots; filter disabled; economy boost to reach targets)
name = "prime-td-auto-advance-80-w"
model = "Qwen/Qwen3-4B-Instruct-2507"
max_steps = 80
batch_size = 64
rollouts_per_example = 16

env_file = ["secrets.env"]

[sampling]
max_tokens = 64
temperature = 0.6

[[env]]
id = "kbediako/prime-td-env"

[env.args.config.difficulty]
max_rounds = 30

[env.args.config.economy]
starting_cash = 1500
end_round_income = 150
starting_lives = 100

[env.args.config.episode]
max_steps = 200

[env.args.config.rewards]
step_penalty = 0.1
invalid_action_penalty = 1.0
life_loss_penalty = 10.0
pop_reward_multiplier = 1.0
macro_round_invariant_penalty = 50.0

[env.args.config.reward_shaping]
delta_reward = true
regret_metric = true

[env.args.config.reward_weights]
format = 0.5
env = 1.0

[env.args.config.observation]
max_action_candidates = 100
max_build_slots = 6
max_towers = 50
max_threats = 5

[env.args.config.dataset]
num_seeds = 512
rollout_steps = 120
policy = "safe_explore"
margin_min = 0 # disables decision filter (kept=0 with margin_min>=1)
safe_explore_upgrade_prob = 0.0

[env.args.config.dataset.snapshots]
mode = "rounds"
rounds = [15, 20]
prep_remaining = 1

[env.args.config.dataset.diagnostics]
dominance = true

[env.args.config.rules]
auto_advance_round = true
prep_actions_per_round = 2
prep_actions_round_scale = 0.0
prep_actions_max = 6
start_round_max_prep_remaining = 1
require_tower_before_start = true
mask_sell = true
require_choose = true
